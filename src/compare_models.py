# src/compare_models.py
import argparse
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
import tensorflow_datasets as tfds
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from keras.models import load_model
import pandas as pd
import time
import os

# ν•κΈ€ ν°νΈ μ„¤μ • (μ΅°μ„±λ―Όλ‹ μ„ νΈλ„μ— λ§μ¶°)
plt.rcParams['font.family'] = 'NanumGothic'
plt.rcParams['axes.unicode_minus'] = False

def preprocess(image, label):
    """EMNIST λ°μ΄ν„°μ…‹μ„ μ¬λ°”λ¥Έ λ°©ν–¥μΌλ΅ λ³€ν™ν•κ³  μ •κ·ν™”ν•©λ‹λ‹¤."""
    image = tf.image.rot90(image, k=3)
    image = tf.image.flip_left_right(image)
    image = tf.cast(image, tf.float32) / 255.0
    return image, label

def load_test_data(num_samples=2000):
    """ν…μ¤νΈ λ°μ΄ν„°λ¥Ό λ΅λ“ν•κ³  μ „μ²λ¦¬ν•©λ‹λ‹¤."""
    print(f"ν…μ¤νΈ λ°μ΄ν„° λ΅λ”© μ¤‘... (μƒν” μ: {num_samples})")
    test_ds = tfds.load('emnist/byclass', split='test', as_supervised=True).map(preprocess)
    
    test_sample = test_ds.take(num_samples)
    x_test_list, y_true_list = [], []
    
    for image, label in test_sample:
        x_test_list.append(image.numpy())
        y_true_list.append(label.numpy())
    
    return np.array(x_test_list), np.array(y_true_list)

def evaluate_model(model, x_test, y_true, model_name):
    """λ¨λΈμ„ ν‰κ°€ν•κ³  μ„±λ¥ μ§€ν‘λ¥Ό λ°ν™ν•©λ‹λ‹¤."""
    print(f"\n{model_name} λ¨λΈ ν‰κ°€ μ¤‘...")
    
    # μ¶”λ΅  μ‹κ°„ μΈ΅μ •
    start_time = time.time()
    y_pred_probs = model.predict(x_test, batch_size=32, verbose=0)
    end_time = time.time()
    
    inference_time = end_time - start_time
    y_pred = np.argmax(y_pred_probs, axis=1)
    
    # μ„±λ¥ μ§€ν‘ κ³„μ‚°
    accuracy = accuracy_score(y_true, y_pred)
    
    # ν΄λμ¤λ³„ μ •ν™•λ„ κ³„μ‚°
    class_names = [str(i) for i in range(10)] + [chr(i) for i in range(65, 91)] + [chr(i) for i in range(97, 123)]
    
    results = {
        'model_name': model_name,
        'accuracy': accuracy,
        'inference_time': inference_time,
        'avg_inference_per_sample': inference_time / len(x_test),
        'y_pred': y_pred,
        'y_pred_probs': y_pred_probs,
        'num_parameters': model.count_params()
    }
    
    print(f"{model_name} κ²°κ³Ό:")
    print(f"  μ •ν™•λ„: {accuracy:.4f}")
    print(f"  μ΄ μ¶”λ΅  μ‹κ°„: {inference_time:.2f}μ΄")
    print(f"  μƒν”λ‹Ή ν‰κ·  μ¶”λ΅  μ‹κ°„: {inference_time/len(x_test)*1000:.2f}ms")
    print(f"  λ¨λΈ νλΌλ―Έν„° μ: {model.count_params():,}")
    
    return results

def create_comparison_plots(results_list, y_true, save_dir='results'):
    """λ‘ λ¨λΈμ„ λΉ„κµν•λ” λ‹¤μ–‘ν• μ‹κ°ν™”λ¥Ό μƒμ„±ν•©λ‹λ‹¤."""
    os.makedirs(save_dir, exist_ok=True)
    class_names = [str(i) for i in range(10)] + [chr(i) for i in range(65, 91)] + [chr(i) for i in range(97, 123)]
    
    # 1. μ„±λ¥ μ§€ν‘ λΉ„κµ λ°”μ°¨νΈ
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    model_names = [r['model_name'] for r in results_list]
    accuracies = [r['accuracy'] for r in results_list]
    inference_times = [r['avg_inference_per_sample']*1000 for r in results_list]  # ms λ‹¨μ„
    param_counts = [r['num_parameters']/1000 for r in results_list]  # K λ‹¨μ„
    
    # μ •ν™•λ„ λΉ„κµ
    bars1 = axes[0, 0].bar(model_names, accuracies, color=['skyblue', 'lightcoral'])
    axes[0, 0].set_title('λ¨λΈλ³„ μ •ν™•λ„ λΉ„κµ', fontsize=14, fontweight='bold')
    axes[0, 0].set_ylabel('μ •ν™•λ„')
    axes[0, 0].set_ylim(0, 1)
    for i, bar in enumerate(bars1):
        height = bar.get_height()
        axes[0, 0].text(bar.get_x() + bar.get_width()/2., height + 0.01,
                       f'{accuracies[i]:.4f}', ha='center', va='bottom', fontweight='bold')
    
    # μ¶”λ΅  μ‹κ°„ λΉ„κµ
    bars2 = axes[0, 1].bar(model_names, inference_times, color=['lightgreen', 'orange'])
    axes[0, 1].set_title('λ¨λΈλ³„ μƒν”λ‹Ή μ¶”λ΅  μ‹κ°„ λΉ„κµ', fontsize=14, fontweight='bold')
    axes[0, 1].set_ylabel('μ¶”λ΅  μ‹κ°„ (ms)')
    for i, bar in enumerate(bars2):
        height = bar.get_height()
        axes[0, 1].text(bar.get_x() + bar.get_width()/2., height + 0.1,
                       f'{inference_times[i]:.2f}ms', ha='center', va='bottom', fontweight='bold')
    
    # νλΌλ―Έν„° μ λΉ„κµ
    bars3 = axes[1, 0].bar(model_names, param_counts, color=['plum', 'wheat'])
    axes[1, 0].set_title('λ¨λΈλ³„ νλΌλ―Έν„° μ λΉ„κµ', fontsize=14, fontweight='bold')
    axes[1, 0].set_ylabel('νλΌλ―Έν„° μ (K)')
    for i, bar in enumerate(bars3):
        height = bar.get_height()
        axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + height*0.02,
                       f'{param_counts[i]:.1f}K', ha='center', va='bottom', fontweight='bold')
    
    # ν¨μ¨μ„± μ§€ν‘ (μ •ν™•λ„/νλΌλ―Έν„°μ)
    efficiency = [acc/param for acc, param in zip(accuracies, param_counts)]
    bars4 = axes[1, 1].bar(model_names, efficiency, color=['lightsteelblue', 'lightsalmon'])
    axes[1, 1].set_title('ν¨μ¨μ„± μ§€ν‘ (μ •ν™•λ„/νλΌλ―Έν„°μ)', fontsize=14, fontweight='bold')
    axes[1, 1].set_ylabel('ν¨μ¨μ„±')
    for i, bar in enumerate(bars4):
        height = bar.get_height()
        axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + height*0.02,
                       f'{efficiency[i]:.6f}', ha='center', va='bottom', fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(f'{save_dir}/model_comparison_metrics.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # 2. ν΄λμ¤λ³„ μ •ν™•λ„ ννΈλ§µ λΉ„κµ
    fig, axes = plt.subplots(1, len(results_list), figsize=(20, 8))
    
    for i, result in enumerate(results_list):
        # ν΄λμ¤λ³„ μ •ν™•λ„ κ³„μ‚°
        class_accuracy = []
        for class_idx in range(62):
            mask = (y_true == class_idx)
            if np.sum(mask) > 0:
                class_acc = np.mean(result['y_pred'][mask] == y_true[mask])
                class_accuracy.append(class_acc)
            else:
                class_accuracy.append(0)
        
        # 10x7 κ·Έλ¦¬λ“λ΅ μ¬λ°°μ—΄ (μ«μ 10κ° + λ€λ¬Έμ 26κ° + μ†λ¬Έμ 26κ°)
        # EMNIST ByClassλ” μ΄ 62κ° ν΄λμ¤ (0-9, A-Z, a-z)
        # 7ν–‰ 9μ—΄ (63μΉΈ) κ·Έλ¦¬λ“μ— λ§μ¶”κΈ° μ„ν•΄ 1κ°μ ν¨λ”© μ¶”κ°€
        # μ•„λ‹λ©΄ 8ν–‰ 8μ—΄ (64μΉΈ) κ·Έλ¦¬λ“μ— 2κ°μ ν¨λ”©μ„ λ„£μ„ μλ„ μμµλ‹λ‹¤.
        accuracy_grid = np.array(class_accuracy + [0]*(63 - len(class_accuracy))).reshape(7, 9)
        # λλ”
        # accuracy_grid = np.array(class_accuracy + [0]*(64 - len(class_accuracy))).reshape(8, 8)
                
        sns.heatmap(accuracy_grid, annot=True, fmt='.3f', cmap='RdYlBu_r',
                   ax=axes[i], cbar_kws={'shrink': 0.8})
        axes[i].set_title(f'{result["model_name"]} ν΄λμ¤λ³„ μ •ν™•λ„', fontsize=12, fontweight='bold')
        axes[i].set_xlabel('ν΄λμ¤ μΈλ±μ¤')
        axes[i].set_ylabel('κ·Έλ£Ή')
    
    plt.tight_layout()
    plt.savefig(f'{save_dir}/class_accuracy_comparison.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # 3. νΌλ™ ν–‰λ ¬ μ°¨μ΄ μ‹κ°ν™”
    if len(results_list) == 2:
        cm1 = confusion_matrix(y_true, results_list[0]['y_pred'])
        cm2 = confusion_matrix(y_true, results_list[1]['y_pred'])
        
        # μ •κ·ν™”λ νΌλ™ ν–‰λ ¬
        cm1_norm = cm1.astype('float') / cm1.sum(axis=1)[:, np.newaxis]
        cm2_norm = cm2.astype('float') / cm2.sum(axis=1)[:, np.newaxis]
        
        # μ°¨μ΄ κ³„μ‚°
        cm_diff = cm2_norm - cm1_norm
        
        plt.figure(figsize=(20, 16))
        sns.heatmap(cm_diff, annot=False, cmap='RdBu_r', center=0,
                   xticklabels=class_names, yticklabels=class_names,
                   cbar_kws={'shrink': 0.8})
        plt.title(f'{results_list[1]["model_name"]} - {results_list[0]["model_name"]} νΌλ™ν–‰λ ¬ μ°¨μ΄\n(μ–‘μ: {results_list[1]["model_name"]}μ΄ λ” μΆ‹μ)', 
                 fontsize=16, fontweight='bold')
        plt.xlabel('μμΈ΅ λΌλ²¨')
        plt.ylabel('μ‹¤μ  λΌλ²¨')
        plt.tight_layout()
        plt.savefig(f'{save_dir}/confusion_matrix_difference.png', dpi=300, bbox_inches='tight')
        plt.close()

def create_summary_report(results_list, save_dir='results'):
    """λ¨λΈ λΉ„κµ μ”μ•½ λ³΄κ³ μ„λ¥Ό μƒμ„±ν•©λ‹λ‹¤."""
    report_path = f'{save_dir}/comparison_report.txt'
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("=" * 60 + "\n")
        f.write("         λ¨λΈ μ„±λ¥ λΉ„κµ λ³΄κ³ μ„\n")
        f.write("=" * 60 + "\n\n")
        
        for i, result in enumerate(results_list, 1):
            f.write(f"{i}. {result['model_name']} λ¨λΈ\n")
            f.write("-" * 40 + "\n")
            f.write(f"μ •ν™•λ„: {result['accuracy']:.4f} ({result['accuracy']*100:.2f}%)\n")
            f.write(f"μ΄ μ¶”λ΅  μ‹κ°„: {result['inference_time']:.2f}μ΄\n")
            f.write(f"μƒν”λ‹Ή ν‰κ·  μ¶”λ΅  μ‹κ°„: {result['avg_inference_per_sample']*1000:.2f}ms\n")
            f.write(f"λ¨λΈ νλΌλ―Έν„° μ: {result['num_parameters']:,}κ°\n")
            f.write(f"ν¨μ¨μ„± μ§€ν‘: {result['accuracy']/(result['num_parameters']/1000):.6f}\n\n")
        
        if len(results_list) == 2:
            r1, r2 = results_list
            f.write("λΉ„κµ λ¶„μ„\n")
            f.write("-" * 40 + "\n")
            acc_diff = r2['accuracy'] - r1['accuracy']
            time_diff = r2['avg_inference_per_sample'] - r1['avg_inference_per_sample']
            param_diff = r2['num_parameters'] - r1['num_parameters']
            
            f.write(f"μ •ν™•λ„ μ°¨μ΄: {acc_diff:+.4f} ({acc_diff*100:+.2f}%p)\n")
            f.write(f"μ¶”λ΅  μ‹κ°„ μ°¨μ΄: {time_diff*1000:+.2f}ms\n")
            f.write(f"νλΌλ―Έν„° μ μ°¨μ΄: {param_diff:+,}κ°\n")
            
            if acc_diff > 0:
                f.write(f"\nβ… {r2['model_name']} λ¨λΈμ΄ μ •ν™•λ„κ°€ λ” λ†’μµλ‹λ‹¤.\n")
            else:
                f.write(f"\nβ… {r1['model_name']} λ¨λΈμ΄ μ •ν™•λ„κ°€ λ” λ†’μµλ‹λ‹¤.\n")
            
            if time_diff < 0:
                f.write(f"β… {r2['model_name']} λ¨λΈμ΄ μ¶”λ΅  μ†λ„κ°€ λ” λΉ λ¦…λ‹λ‹¤.\n")
            else:
                f.write(f"β… {r1['model_name']} λ¨λΈμ΄ μ¶”λ΅  μ†λ„κ°€ λ” λΉ λ¦…λ‹λ‹¤.\n")
    
    print(f"\nπ“ μƒμ„Έ λ³΄κ³ μ„κ°€ {report_path}μ— μ €μ¥λμ—μµλ‹λ‹¤.")

def main():
    parser = argparse.ArgumentParser(description='λ‘ λ¨λΈμ μ„±λ¥μ„ λΉ„κµν•©λ‹λ‹¤.')
    parser.add_argument('--model1', type=str, required=True, help='μ²« λ²μ§Έ λ¨λΈ κ²½λ΅')
    parser.add_argument('--model2', type=str, required=True, help='λ‘ λ²μ§Έ λ¨λΈ κ²½λ΅')
    parser.add_argument('--samples', type=int, default=2000, help='ν‰κ°€ν•  μƒν” μ (κΈ°λ³Έκ°’: 2000)')
    parser.add_argument('--output', type=str, default='results', help='κ²°κ³Ό μ €μ¥ λ””λ ‰ν† λ¦¬')
    args = parser.parse_args()
    
    # κ²°κ³Ό λ””λ ‰ν† λ¦¬ μƒμ„±
    os.makedirs(args.output, exist_ok=True)
    
    # λ°μ΄ν„° λ΅λ“
    x_test, y_true = load_test_data(args.samples)
    
    # λ¨λΈλ“¤ λ΅λ“ λ° ν‰κ°€
    results_list = []
    
    for model_path in [args.model1, args.model2]:
        if not os.path.exists(model_path):
            print(f"β λ¨λΈ νμΌμ„ μ°Ύμ„ μ μ—†μµλ‹λ‹¤: {model_path}")
            return
        
        print(f"\nλ¨λΈ λ΅λ”© μ¤‘: {model_path}")
        model = load_model(model_path)
        model_name = os.path.basename(model_path).replace('.keras', '').replace('_emnist', '')
        
        result = evaluate_model(model, x_test, y_true, model_name)
        results_list.append(result)
    
    # λΉ„κµ μ‹κ°ν™” λ° λ³΄κ³ μ„ μƒμ„±
    print(f"\nπ“ λΉ„κµ μ‹κ°ν™” μƒμ„± μ¤‘...")
    create_comparison_plots(results_list, y_true, args.output)
    create_summary_report(results_list, args.output)
    
    print(f"\nπ‰ λ¨λΈ λΉ„κµ μ™„λ£!")
    print(f"κ²°κ³Ό νμΌλ“¤μ΄ '{args.output}' λ””λ ‰ν† λ¦¬μ— μ €μ¥λμ—μµλ‹λ‹¤:")
    print(f"  - model_comparison_metrics.png (μ„±λ¥ μ§€ν‘ λΉ„κµ)")
    print(f"  - class_accuracy_comparison.png (ν΄λμ¤λ³„ μ •ν™•λ„)")
    print(f"  - confusion_matrix_difference.png (νΌλ™ν–‰λ ¬ μ°¨μ΄)")
    print(f"  - comparison_report.txt (μƒμ„Έ λ³΄κ³ μ„)")

if __name__ == "__main__":
    main()
